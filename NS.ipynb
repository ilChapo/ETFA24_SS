{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "531e7e05",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cpu\n"
     ]
    }
   ],
   "source": [
    "# Setting seeds to try and ensure we have the same results - this is not guaranteed across PyTorch releases.\n",
    "import torch\n",
    "import sys\n",
    "import time\n",
    "import control\n",
    "import os\n",
    "from torch.optim import Adam\n",
    "torch.manual_seed(0)\n",
    "torch.backends.cudnn.deterministic = True\n",
    "torch.backends.cudnn.benchmark = False\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import numpy as np\n",
    "from model_archs import LSTM_FF\n",
    "from model_archs import LSTM_FF_dropout\n",
    "import scipy.io\n",
    "np.random.seed(0)\n",
    "\n",
    "import h5py\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import KFold\n",
    "\n",
    "import math\n",
    "from sklearn.metrics import r2_score\n",
    "\n",
    "proces_params = {} \n",
    "\n",
    "# PARAMETRIZATION\n",
    "sampl_time = 15/1440 #as in MATLAB files\n",
    "learning_rate = 1e-3\n",
    "use_prev_prediction = False\n",
    "batch_size = 512 \n",
    "if use_prev_prediction:\n",
    "    test_batch_size = 1\n",
    "else:\n",
    "    test_batch_size = batch_size\n",
    "num_epochs = 100\n",
    "test_size = 0.15\n",
    "early_stop_thresh = 30\n",
    "num_folds = 5\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "train_loader_shuffle = False\n",
    "kfold_shuffle = True\n",
    "prev2 = False\n",
    "break_first_fold = False #if stopping after first fold training\n",
    "nodrop = False # dropout in model yes or no\n",
    "custom_loss = False\n",
    "\n",
    "print(device)\n",
    "\n",
    "setpoint_signal = './ETROTLE_Dataset_ETFA24_MAT_files/reference/' \n",
    "first_order_processes_mat = './ETROTLE_Dataset_ETFA24_MAT_files/PID_tuning_params_1stO_processes.mat'\n",
    "pids_mat_dir = './ETROTLE_Dataset_ETFA24_MAT_files/1st_order/' \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "218cf9db",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def get_inputs_output (process, process_folder):\n",
    "    \"\"\"\n",
    "    Get training data from the .mat files of a certain tuned PID for a certain process.\n",
    "    Generate input and output vectors.\n",
    "    \"\"\"\n",
    "    mesura_process = f'mesura_PID_{process}'\n",
    "    actuacio_process = f'actuacio_PID_{process}'\n",
    "    \n",
    "    print(mesura_process)\n",
    "    \n",
    "    P1_fold = setpoint_signal \n",
    "\n",
    "    print(P1_fold)\n",
    "    \n",
    "    So5sim = np.array(scipy.io.loadmat(P1_fold + 'referencia_PID_P1.mat')['referencia_PID_P1']).reshape((-1, 1))\n",
    "    So5out = np.array(scipy.io.loadmat(process_folder + mesura_process +'.mat')[mesura_process]).reshape((-1, 1))\n",
    "    kla = np.array(scipy.io.loadmat(process_folder + actuacio_process + '.mat')[actuacio_process]).reshape((-1, 1))\n",
    "    \n",
    "    error = So5sim - So5out\n",
    "\n",
    "    # The previously predicted value as an extra input of the model (NARX models in ANNs)\n",
    "    kla_delayed = np.vstack([kla[0, :], kla[0:len(kla)-1, :]])\n",
    "\n",
    "    inputs = np.hstack((So5sim, So5out, error, kla_delayed))\n",
    "    output = kla.reshape((-1,1))\n",
    "\n",
    "    if prev2:\n",
    "        kla_delayed2 = np.vstack([kla[0, :], kla[1,:], kla[0:len(kla)-2, :]])\n",
    "        inputs = np.hstack((So5sim, So5out, error, kla_delayed, kla_delayed2))        \n",
    "    \n",
    "    return inputs, output\n",
    "\n",
    "def get_datasets_dataloaders (inputs, output, bs=batch_size, testbs=test_batch_size):\n",
    "    # Split the data into training and testing sets\n",
    "    train_inputs, test_inputs, train_output, test_output = train_test_split(inputs, output, test_size=test_size, random_state=42)\n",
    "\n",
    "    # Standardize the data separately for training and testing, use scaler for training for scaling test data\n",
    "    scaler = StandardScaler()\n",
    "    train_inputs_standardized = scaler.fit_transform(train_inputs)\n",
    "    test_inputs_standardized = scaler.transform(test_inputs)\n",
    "\n",
    "    #SCALE OUTPUT DATA\n",
    "\n",
    "    scaler_out = StandardScaler()\n",
    "    train_outputs_standardized = scaler_out.fit_transform(train_output)\n",
    "    test_outputs_standardized = scaler_out.transform(test_output)\n",
    "\n",
    "    # Convert to PyTorch tensors\n",
    "    train_inputs_tensor = torch.tensor(train_inputs_standardized, dtype=torch.float32)\n",
    "    train_output_tensor = torch.tensor(train_outputs_standardized, dtype=torch.float32)\n",
    "\n",
    "    test_inputs_tensor = torch.tensor(test_inputs_standardized, dtype=torch.float32)\n",
    "    test_output_tensor = torch.tensor(test_outputs_standardized, dtype=torch.float32)\n",
    "\n",
    "    # Create PyTorch datasets\n",
    "    train_dataset = TensorDataset(train_inputs_tensor, train_output_tensor)\n",
    "    test_dataset = TensorDataset(test_inputs_tensor, test_output_tensor)\n",
    "\n",
    "    # Create PyTorch DataLoaders\n",
    "    train_dataloader = DataLoader(train_dataset, batch_size=bs, shuffle=train_loader_shuffle)\n",
    "    test_dataloader = DataLoader(test_dataset, batch_size=testbs, shuffle=False)\n",
    "    \n",
    "    print(\"Scaler data\")\n",
    "    print(scaler.mean_)\n",
    "    print(scaler.scale_)\n",
    "\n",
    "    for batch, (inputs, outputs) in enumerate(train_dataloader, 1):\n",
    "        print(batch, inputs.size(), outputs.size())\n",
    "        break\n",
    "\n",
    "    return train_dataset, test_dataset, train_dataloader, test_dataloader, scaler, scaler_out\n",
    "\n",
    "def plot_and_save_results(tr_losses, te_losses, suffix, experiment_num = 1):\n",
    "    plt.figure(figsize=(10, 3))\n",
    "\n",
    "    plt.subplot(1, 2, 1)\n",
    "    plt.plot(tr_losses, label='Training Loss', color='blue')\n",
    "    plt.plot(te_losses, label='Testing Loss', color='red')\n",
    "    plt.title(f'Training and Testing Loss Over Epochs - Experiment {suffix}')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.legend()  \n",
    "    plt.tight_layout()\n",
    "    \n",
    "    # Show the plot\n",
    "    file_path = f'Results/Plots/Loss_plot_{process}_{suffix}.png'\n",
    "    # Save the plot\n",
    "    plt.savefig(file_path)\n",
    "    \n",
    "    plt.show()\n",
    "\n",
    "def count_parameters(model):\n",
    "    return sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "\n",
    "def mean_absolute_error(output, target):\n",
    "    return torch.mean(torch.abs(output - target))\n",
    "\n",
    "def mean_absolute_percentage_error(output, target):\n",
    "    return torch.mean(torch.abs((target - output) / target)) * 100\n",
    "\n",
    "def checkpoint(model, filename):\n",
    "    torch.save(model.state_dict(), filename)\n",
    "    \n",
    "def resume(model, filename):\n",
    "    model.load_state_dict(torch.load(filename))\n",
    "    \n",
    "def init_weights(m):\n",
    "    if isinstance(m, torch.nn.LSTM):\n",
    "        m.reset_parameters()\n",
    "    if isinstance(m, torch.nn.Linear):\n",
    "        m.reset_parameters()    \n",
    "        \n",
    "def get_mesura_proces_pidonline(denorm_actuacio, num1, den1, den2, den3, delay, sampling_time = sampl_time, x0 = None):\n",
    "    \"\"\"\n",
    "    Simulate and return process output based on previous state and process dynamics.\n",
    "    \"\"\"\n",
    "    y_out_list = []\n",
    "\n",
    "    numerator = [num1]\n",
    "    denominator = [den1, den2, den3]\n",
    "\n",
    "    # Create the transfer function\n",
    "    transfer_function = control.TransferFunction(numerator, denominator)\n",
    "\n",
    "    #PADÃ‰\n",
    "    Tau = delay\n",
    "    N = 10\n",
    "    [num_pade, den_pade] = control.pade(Tau, N )\n",
    "    Hpade = control.TransferFunction(num_pade, den_pade)\n",
    "\n",
    "    # Apply the time delay\n",
    "    system_with_delay = control.series(transfer_function, Hpade)\n",
    "\n",
    "    #CONVERT TO STATE SPACE\n",
    "    ss = control.tf2ss(system_with_delay)\n",
    "\n",
    "    sample_time = sampling_time\n",
    "    batch_size = len(denorm_actuacio)\n",
    "\n",
    "    # Define the time vector\n",
    "    t = np.linspace(0, (batch_size - 1) * sample_time, batch_size)         \n",
    "    \n",
    "    if x0 is None:\n",
    "        x0 = [0 for i in range(len(ss.A))]\n",
    "        \n",
    "    u = denorm_actuacio\n",
    "    t_out, y_out, x_out = control.forced_response(ss, T=t, U=u, X0=x0, return_x=True)\n",
    "    #print(u,t_out,y_out)\n",
    "    #print(\"X0\", x0)\n",
    "    #print(\"xout\", x_out)\n",
    "    return y_out.tolist(), t_out, x_out[:,-1] #LIST OF MEASURES OF THE BATCH, LIST OF X0 OF BATCH INSTANCES\n",
    "\n",
    "        \n",
    "def IAE_calculator (inputs_iae, model, scaler, scaler_out, pp = \"A\"):\n",
    "    \"\"\"\n",
    "    One by one simulation pipeline with network recursive predictions over the reference signal,\n",
    "    365 time, 35041 instances. An instance every 0,010416369 seconds.\n",
    "    Return control actuations of the model + errors against ground truth.\n",
    "    \"\"\"\n",
    "    actuacions = []\n",
    "    errors = []\n",
    "\n",
    "    model.to(\"cpu\")\n",
    "    model.eval()\n",
    "\n",
    "    setpoint = inputs_iae[:,0]\n",
    "    inputs_tensor = torch.empty(inputs_iae[0].shape)\n",
    "    inputs_tensor = inputs_tensor.unsqueeze(0).unsqueeze(0) #1,1,4 shape\n",
    "    counter = 1\n",
    "    X0 = None\n",
    "    hidden = None\n",
    "\n",
    "    if prev2:\n",
    "        prediction = inputs_iae[0,4]\n",
    "    \n",
    "    for ref in setpoint: \n",
    "        if counter == 1:\n",
    "            inputs_0_norm = (inputs_iae[0] - scaler.mean_) / scaler.scale_\n",
    "            inputs_tensor = torch.tensor(inputs_0_norm ,dtype=torch.float32).to(\"cpu\")\n",
    "            inputs_tensor = inputs_tensor.unsqueeze(0).unsqueeze(0)\n",
    "\n",
    "            denorm_actuacio_previa = np.array([inputs_iae[0,3]])[0]\n",
    "            \n",
    "        else:\n",
    "            ref_norm = (ref - scaler.mean_[0]) / scaler.scale_[0]\n",
    "            inputs_tensor[:,:,0] = ref_norm\n",
    "            inputs_tensor[:,:,3] = prediction\n",
    "            #NEW\n",
    "            if prev2:\n",
    "                inputs_tensor[:,:,4] = prev2_prediction\n",
    "            denorm_actuacio = scaler_out.scale_ * output_prev_ann + scaler_out.mean_\n",
    "            actuacions.append(denorm_actuacio)\n",
    "\n",
    "            denorm_actuacio = [denorm_actuacio_previa, denorm_actuacio[0]]\n",
    "            mesura_lstm, _ , X0 = get_mesura_proces_pidonline(denorm_actuacio, pp[0], pp[1], pp[2], pp[3], pp[4], x0 = X0)\n",
    "            denorm_actuacio_previa = denorm_actuacio[-1]\n",
    "            mesura_lstm = torch.tensor(mesura_lstm[-1]) \n",
    "\n",
    "            #Normalize mesura_lstm for putting in inputs\n",
    "            mesura_lstm_norm = (mesura_lstm - scaler.mean_[1]) / scaler.scale_[1]\n",
    "            inputs_tensor[:,:,1] = mesura_lstm_norm\n",
    "            \n",
    "            #error is denorm ref-mesura and then norm\n",
    "            error = ref - mesura_lstm\n",
    "            errors.append(ref - mesura_lstm.item())\n",
    "            inputs_tensor[:,:,2] = (error - scaler.mean_[2]) / scaler.scale_[2]\n",
    "            \n",
    "        with torch.no_grad():\n",
    "            if prev2:\n",
    "                prev2_prediction = prediction\n",
    "            \n",
    "            prediction = model(inputs_tensor)\n",
    "            predictions_list = [elem[0] for elem in prediction]\n",
    "\n",
    "            predictions = torch.tensor(predictions_list)\n",
    "            output_prev_ann = predictions.numpy().tolist()\n",
    "        counter +=1\n",
    "        \n",
    "        #print(counter, inputs_tensor, prediction)\n",
    "    return actuacions, errors  \n",
    "    \n",
    "class ClassicMSELoss(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super(ClassicMSELoss, self).__init__()\n",
    "\n",
    "    def forward(self, predictions, targets, error, batch_size):\n",
    "        # Compute the standard MSE loss\n",
    "        loss = torch.mean((predictions - targets) ** 2)\n",
    "        return loss\n",
    "\n",
    "\n",
    "class CustomMSELoss(torch.nn.Module):\n",
    "    \"\"\"\n",
    "    Created for testing purposes, not used in practicality.\n",
    "    \"\"\"\n",
    "    def __init__(self):\n",
    "        super(CustomMSELoss, self).__init__()\n",
    "        self.iterations = 0\n",
    "        self.total_integral_error = 0.0\n",
    "\n",
    "    def forward(self, predictions, targets, error, batch_size):\n",
    "        # Compute the standard MSE loss\n",
    "        mse_loss = torch.mean((predictions - targets) ** 2)\n",
    "        \n",
    "        # Compute the integral of the error and accumulate it\n",
    "        self.total_integral_error += torch.sum(error)\n",
    "        \n",
    "        # Update the number of iterations\n",
    "        self.iterations += batch_size\n",
    "        \n",
    "        # Calculate the integral error term\n",
    "        #integral_error_term = self.total_integral_error / self.iterations\n",
    "        integral_error_term = error\n",
    "        #print(self.iterations, self.total_integral_error, integral_error_term)\n",
    "\n",
    "        # Weight for the integral error term\n",
    "        weight = 10  # Adjust this parameter as desired\n",
    "        \n",
    "        # Add the integral error term to the MSE loss\n",
    "        loss = mse_loss + weight * integral_error_term\n",
    "        \n",
    "        return loss\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "0165de10",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def train_model (model, train_dataset, transfer=False, hdf5_paths=\"\"):\n",
    "    \"\"\"\n",
    "    Traditional batch-epoch based training of the network with the train_dataset\n",
    "    data of the PID behavior. Early stopping inside folds and checkpointing of \n",
    "    best model of the Kfold training. Return model and loss signals.\n",
    "    \n",
    "    transfer=True when starting point from an existing model on path = hdf5_paths.\n",
    "    \"\"\"\n",
    "    print(model.to(device))\n",
    "\n",
    "    #optimizer = Adam(model.parameters(), lr=learning_rate, weight_decay=0.001)\n",
    "    optimizer = Adam(model.parameters(), lr=learning_rate)\n",
    "\n",
    "    if custom_loss:\n",
    "        criterion = CustomMSELoss()\n",
    "    else:\n",
    "        criterion = ClassicMSELoss()  \n",
    "    print(f'criterion is {str(criterion)}')\n",
    "\n",
    "    kf = KFold(n_splits=num_folds, shuffle=kfold_shuffle)\n",
    "    \n",
    "    best_val = sys.maxsize\n",
    "\n",
    "    start_time = time.time()\n",
    "    fold_val_loss = []\n",
    "    \n",
    "    tr_loss = []\n",
    "    te_loss = []\n",
    "    \n",
    "    # Loop over folds\n",
    "    for fold, (train_idx, val_idx) in enumerate(kf.split(train_dataset)):\n",
    "        print(f\"\\nFold {fold + 1}/{num_folds}\")\n",
    "\n",
    "        if(transfer == False):\n",
    "            print(\"Reset model parameters no transfer\")\n",
    "            model.apply(init_weights)\n",
    "\n",
    "        else:\n",
    "            print(\"Transfer true, load weights\")\n",
    "            model = load_model_from_h5(model, hdf5_paths)\n",
    "            model.to(device)\n",
    "\n",
    "        # Create dataloaders for this fold\n",
    "        train_fold_dataset = torch.utils.data.Subset(train_dataset, train_idx)\n",
    "        val_fold_dataset = torch.utils.data.Subset(train_dataset, val_idx)\n",
    "\n",
    "        train_fold_loader = DataLoader(train_fold_dataset, batch_size=batch_size, shuffle=train_loader_shuffle)\n",
    "        val_fold_loader = DataLoader(val_fold_dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "        best_val_e = sys.maxsize\n",
    "        best_epoch = 0\n",
    "\n",
    "        # Training loop for each fold\n",
    "        for e in range(num_epochs):\n",
    "            batch_loss = 0\n",
    "            \n",
    "            #Reboot loss per epoch\n",
    "            if custom_loss:\n",
    "                criterion = CustomMSELoss()\n",
    "            else:\n",
    "                criterion = ClassicMSELoss()  \n",
    "\n",
    "            # Training phase\n",
    "            model.train()\n",
    "            for batch, (inputs, outputs) in enumerate(train_fold_loader, 1):\n",
    "                inputs = inputs.to(device).unsqueeze(1)\n",
    "                outputs = outputs.to(device)\n",
    "                optimizer.zero_grad()\n",
    "                predictions = model(inputs)\n",
    "                loss = criterion(predictions, outputs, inputs[:,0,2], batch_size)\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "\n",
    "                batch_loss += loss.item()\n",
    "                #print(f'Epoch({e + 1}/{num_epochs}: Batch number({batch}/{len(train_fold_loader)}) : Batch loss : {loss.item()}')\n",
    "            print(f'Epoch({e + 1}/{num_epochs})')\n",
    "            print(f'Training loss : {batch_loss / len(train_fold_loader)}')\n",
    "            tr_loss.append(batch_loss / len(train_fold_loader))\n",
    "            \n",
    "            criterion = torch.nn.MSELoss() #default choice\n",
    "            \n",
    "            # Validation phase\n",
    "            model.eval()\n",
    "            val_loss = 0\n",
    "            with torch.no_grad():\n",
    "                for batch_val, (inputs, outputs) in enumerate(val_fold_loader, 1):\n",
    "                    inputs = inputs.to(device).unsqueeze(1)\n",
    "                    outputs = outputs.to(device)\n",
    "                    predictions = model(inputs)\n",
    "                    loss = criterion(predictions, outputs)\n",
    "                    \n",
    "                    val_loss += loss.item()\n",
    "                print(f'Validation loss : {val_loss / len(val_fold_loader)}')\n",
    "                te_loss.append(val_loss / len(val_fold_loader))\n",
    "\n",
    "                val_epoch = val_loss / len(val_fold_loader)\n",
    "\n",
    "                #Best model checkpointing\n",
    "                if(val_epoch < best_val):\n",
    "                    print(f'Checkpointing new best model in epoch {e+1} with fold {fold+1}')\n",
    "                    checkpoint(model, f\"./Temp_Models/best_model.pkl\")\n",
    "                    best_val = val_epoch\n",
    "\n",
    "            #Check val loss of epoch for early stopping\n",
    "            if val_epoch < best_val_e:\n",
    "                best_val_e = val_epoch\n",
    "                best_epoch = e\n",
    "\n",
    "            #Early stopping\n",
    "            if e - best_epoch > early_stop_thresh:\n",
    "                print(f'Early stopped training at epoch {e+1}')\n",
    "                break  # terminate the training loop\n",
    "\n",
    "        #At the end of all the epochs\n",
    "        print(f'Checkpointing epoch {e+1} and fold {fold+1} ')\n",
    "        checkpoint(model, f\"./Temp_Models/fold-{fold+1}.pkl\")\n",
    "\n",
    "        fold_val_loss.append(best_val_e)\n",
    "        \n",
    "        if break_first_fold:\n",
    "            print(\"BREAKING FIRST FOLD\")\n",
    "            break\n",
    "        \n",
    "    print(f'\\nFinal, best val error: {best_val}')\n",
    "    print(f'Avg val loss CV is: {sum(fold_val_loss)/len(fold_val_loss)}')\n",
    "\n",
    "    end_time = time.time()\n",
    "    elapsed_time = end_time - start_time\n",
    "    minutes = int(elapsed_time // 60)\n",
    "    seconds = int(elapsed_time % 60)\n",
    "\n",
    "    print(f\"Elapsed Time: {minutes} minutes and {seconds} seconds\")\n",
    "    \n",
    "    return model, tr_loss, te_loss\n",
    "\n",
    "    \n",
    "def evaluate_model (model, test_dataloader, use_prediction = False):\n",
    "    \"\"\"\n",
    "    Evaluate model on test set and plot it with metrics, against real \n",
    "    PID ground_truth or with previous actuation fed in input.\n",
    "    \"\"\"\n",
    "    \n",
    "    device = 'cpu'\n",
    "    model.to(device)\n",
    "    model.eval()\n",
    "    test_loss = 0\n",
    "    predictions_all = []\n",
    "    outputs_all = []\n",
    "    criterion = torch.nn.MSELoss()\n",
    "    prev_prediction = None\n",
    "    \n",
    "    print (\"Evaluation with real labels PID\")\n",
    "\n",
    "    with torch.no_grad(): #deactivate dropout regularization, if there is\n",
    "        for batch_val, (inputs, outputs) in enumerate(test_dataloader, 1):\n",
    "            inputs = inputs.to(device).unsqueeze(1)\n",
    "            outputs = outputs.to(device)\n",
    "            \n",
    "            predictions = model(inputs)\n",
    "\n",
    "            loss = criterion(predictions, outputs)\n",
    "            test_loss += loss.item()\n",
    "            predictions_all.append(predictions.cpu().numpy())\n",
    "            outputs_all.append(outputs.cpu().numpy())\n",
    "            \n",
    "    predictions_all = np.concatenate(predictions_all, axis=0)\n",
    "    outputs_all = np.concatenate(outputs_all, axis=0)\n",
    "    r2 = r2_score(outputs_all, predictions_all)\n",
    "    mape = np.mean(np.abs((outputs_all - predictions_all) / outputs_all)) * 100\n",
    "    rmse = math.sqrt(test_loss / len(test_dataloader))\n",
    "    \n",
    "    print(f'RMSE: {rmse}')\n",
    "    print(f'R^2: {r2}')\n",
    "    print(f'MAPE: {mape}%')\n",
    "    \n",
    "    # Plot predictions vs. outputs\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.plot(outputs_all, 'b-', label='Actual')  # Blue solid line for actual\n",
    "    plt.plot(predictions_all, 'r--', label='Predicted')  # Red dashed line for predicted\n",
    "    plt.xlabel('Sample')\n",
    "    plt.ylabel('Value')\n",
    "    plt.title('Actual vs Predicted')\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "def save_model_as_h5(model, process, suffix):\n",
    "    state_dict = model.state_dict()\n",
    "\n",
    "    with h5py.File(f'./Temp_Models/model_weights_{process}_{suffix}.h5', 'w') as f:\n",
    "        # Save each tensor in the state_dict as a dataset in the HDF5 file\n",
    "        for key, value in state_dict.items():\n",
    "            f.create_dataset(key, data=value.numpy())\n",
    "\n",
    "def load_model_from_h5 (model, path):\n",
    "    # Load the weights from the HDF5 file\n",
    "    with h5py.File(path, 'r') as f:\n",
    "        # Iterate through the layers in your PyTorch model\n",
    "        for name, param in model.named_parameters():\n",
    "            # Check if the layer name exists in the HDF5 file\n",
    "            if name in f:\n",
    "                # Load the weights from the HDF5 file to the PyTorch model\n",
    "                param.data = torch.tensor(f[name][:])\n",
    "    #Whole model trainable\n",
    "    for param in model.parameters():\n",
    "        param.requires_grad = True\n",
    "    \n",
    "    return model\n",
    "                       \n",
    "            \n",
    "def train_and_save_model (process, process_folder, suffix, third=False, transfer_bool = False,\n",
    "                          transfer_suffix = \"\", process_from_transfer = \" \"):\n",
    "    \"\"\"\n",
    "    Train and save model (from scratch or transferring weights), evaluate it against PID,\n",
    "    calculate errors, show them and save plots and metrics.\n",
    "    \"\"\"\n",
    "    inputs, output = get_inputs_output(process, process_folder)\n",
    "            \n",
    "    train_dataset, test_dataset, train_dataloader, test_dataloader2, scaler, scaler_out = get_datasets_dataloaders (inputs, output)\n",
    "\n",
    "    if nodrop:\n",
    "        model = LSTM_FF()\n",
    "    else:\n",
    "        model = LSTM_FF_dropout()\n",
    " \n",
    "    #transfer\n",
    "    hdf5_path = \"\"\n",
    "    if transfer_bool:\n",
    "        hdf5_path = f'Temp_Models/model_weights_{process}_{suffix}.h5'\n",
    "\n",
    "    #Train model, best model will be in best_model.pkl as well\n",
    "    model_trained, tr_loss, te_loss = train_model(model, train_dataset, transfer= transfer_bool, hdf5_paths=hdf5_path)\n",
    "\n",
    "    model_state_dict = torch.load(\"./Temp_Models/best_model.pkl\")\n",
    "    model.load_state_dict(model_state_dict)\n",
    "    \n",
    "    print(\"suffix before\", suffix)\n",
    "    \n",
    "    if transfer_bool:\n",
    "        suffix = suffix + f'_transfer_{transfer_suffix}'\n",
    "    \n",
    "    print(\"suffix after\", suffix)\n",
    "    \n",
    "    plot_and_save_results(tr_loss, te_loss, suffix)\n",
    "\n",
    "    #Evaluate over PID ground-truth input-outputs\n",
    "    evaluate_model(model, test_dataloader2, use_prediction=False)\n",
    "\n",
    "    print(f'IAE and ISE of model for {process}')\n",
    "    actuacions, errors = IAE_calculator (inputs, model, scaler, scaler_out, proces_params[process])\n",
    "    # Flatten each array and concatenate into a single list\n",
    "    actuacions_list = [number for array in actuacions for number in array.flatten()]\n",
    "    \n",
    "    # Calculate the integral absolute error\n",
    "    IAE = sum(abs(error) for error in errors) * sample_time\n",
    "    print(\"Integral Absolute Error (IAE):\", IAE)\n",
    "    # Calculate the integral squared error\n",
    "    ISE = sum(error**2 for error in errors) * sample_time\n",
    "    print(\"Integral Squared Error (ISE):\", ISE)\n",
    "    plt.figure(figsize=(10, 6))  \n",
    "    plt.plot(output[1:], label='PID', linewidth=1)  \n",
    "    plt.plot(actuacions_list, label='ANN', linewidth=1) \n",
    "    plt.xlabel('Sample')\n",
    "    plt.ylabel('Control signal')\n",
    "    plt.legend()\n",
    "    plt.title('Comparison of PID and ANN control signal')\n",
    "    # Show plot  \n",
    "    #NEW\n",
    "    \n",
    "    RMSE = np.sqrt(np.mean((np.array(output[1:]) - np.array(actuacions)) ** 2))\n",
    "    print(\"Root Mean Squared Error (RMSE):\", RMSE)\n",
    "        \n",
    "    # Define the file path to save the plot\n",
    "    file_path = f'./Results/Plots/IAE_ISE_plot_{process}_{suffix}.png'\n",
    "    # Save the plot\n",
    "    plt.savefig(file_path, dpi=200)\n",
    "    \n",
    "    values_file_path = f'./Results/IAE_ISE_{suffix}.txt'\n",
    "    \n",
    "    if not os.path.exists(values_file_path):\n",
    "        with open(values_file_path, 'w') as f:\n",
    "            f.write(f'Process: {process}\\n')\n",
    "            # Write IAE and ISE values\n",
    "            f.write(f'IAE: {IAE}\\nISE: {ISE}\\n')\n",
    "            # Write process variable\n",
    "            f.write(f'RMSE: {RMSE} \\n')\n",
    "            # Write process variable\n",
    "    else:\n",
    "        with open(values_file_path, 'a') as f:\n",
    "            f.write(f'Process: {process}\\n')\n",
    "            # Write IAE and ISE values\n",
    "            f.write(f'IAE: {IAE}\\nISE: {ISE}\\n')\n",
    "            # Write process variable\n",
    "            f.write(f'RMSE: {RMSE} \\n')\n",
    "    \n",
    "    \n",
    "    plt.show()    \n",
    "\n",
    "    if transfer_bool:\n",
    "        save_model_as_h5(model, process_from_transfer, suffix)\n",
    "    \n",
    "        pkl_path = f'./Temp_Models/model_weights_{process_from_transfer}_{suffix}.pkl'\n",
    "    # Save the PyTorch model as a .pkl file\n",
    "        torch.save(model.state_dict(), pkl_path)\n",
    "    else:\n",
    "        save_model_as_h5(model, process, suffix)\n",
    "    \n",
    "        pkl_path = f'./Temp_Models/model_weights_{process}_{suffix}.pkl'\n",
    "    # Save the PyTorch model as a .pkl file\n",
    "        torch.save(model.state_dict(), pkl_path)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f55d822-1a7a-40d3-9212-3585e91cf641",
   "metadata": {},
   "source": [
    "### Execution --> train NS models, 100 epochs, loop for 1-10 FOPDT processes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "3e90f856-4048-40c7-93a5-a9249ba832d8",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "process 1\n",
      "process_params['process'] [1.4, 0, 1.2, 1, 0.13]\n",
      "mesura_PID_1\n",
      "./ETROTLE_Dataset_ETFA24_MAT_files/reference/\n",
      "Scaler data\n",
      "[9.91231767e-01 9.90758943e-01 4.72824175e-04 7.09528959e-01]\n",
      "[0.39851616 0.40047368 0.07421704 0.35155008]\n",
      "1 torch.Size([512, 4]) torch.Size([512, 1])\n",
      "LSTM_FF_dropout(\n",
      "  (lstm1): LSTM(4, 100, batch_first=True)\n",
      "  (lstm2): LSTM(100, 50, batch_first=True)\n",
      "  (dropout_lstm): Dropout(p=0.25, inplace=False)\n",
      "  (fc1): Linear(in_features=50, out_features=25, bias=True)\n",
      "  (relu1): ReLU()\n",
      "  (dropout_dense): Dropout(p=0.25, inplace=False)\n",
      "  (output_layer): Linear(in_features=25, out_features=1, bias=True)\n",
      ")\n",
      "criterion is ClassicMSELoss()\n",
      "\n",
      "Fold 1/5\n",
      "Reset model parameters no transfer\n",
      "Epoch(1/100)\n",
      "Training loss : 0.7932291645952996\n",
      "Validation loss : 0.3788617129127185\n",
      "Checkpointing new best model in epoch 1 with fold 1\n",
      "Epoch(2/100)\n",
      "Training loss : 0.164494025025596\n",
      "Validation loss : 0.07237972551956773\n",
      "Checkpointing new best model in epoch 2 with fold 1\n",
      "Epoch(3/100)\n",
      "Training loss : 0.07016836384192426\n",
      "Validation loss : 0.025719237979501486\n",
      "Checkpointing new best model in epoch 3 with fold 1\n",
      "Epoch(4/100)\n",
      "Training loss : 0.049180900559146354\n",
      "Validation loss : 0.014293290791101754\n",
      "Checkpointing new best model in epoch 4 with fold 1\n",
      "Epoch(5/100)\n",
      "Training loss : 0.0402436005942365\n",
      "Validation loss : 0.009850365129144242\n",
      "Checkpointing new best model in epoch 5 with fold 1\n",
      "Epoch(6/100)\n",
      "Training loss : 0.0411050510295528\n",
      "Validation loss : 0.0075640086239824695\n",
      "Checkpointing new best model in epoch 6 with fold 1\n",
      "Epoch(7/100)\n",
      "Training loss : 0.03799260348240112\n",
      "Validation loss : 0.00549024836315463\n",
      "Checkpointing new best model in epoch 7 with fold 1\n",
      "Epoch(8/100)\n",
      "Training loss : 0.03648267504065595\n",
      "Validation loss : 0.006579969505158563\n",
      "Epoch(9/100)\n",
      "Training loss : 0.03602437508550096\n",
      "Validation loss : 0.0053619516353743775\n",
      "Checkpointing new best model in epoch 9 with fold 1\n",
      "Epoch(10/100)\n",
      "Training loss : 0.034127303815268455\n",
      "Validation loss : 0.004316001800665011\n",
      "Checkpointing new best model in epoch 10 with fold 1\n",
      "Epoch(11/100)\n",
      "Training loss : 0.03383186360464451\n",
      "Validation loss : 0.002895636639247338\n",
      "Checkpointing new best model in epoch 11 with fold 1\n",
      "Epoch(12/100)\n",
      "Training loss : 0.034234476018142196\n",
      "Validation loss : 0.0033344197242210307\n",
      "Epoch(13/100)\n",
      "Training loss : 0.034449885183192315\n",
      "Validation loss : 0.004085315061577906\n",
      "Epoch(14/100)\n",
      "Training loss : 0.03185698774425273\n",
      "Validation loss : 0.002476046676747501\n",
      "Checkpointing new best model in epoch 14 with fold 1\n",
      "Epoch(15/100)\n",
      "Training loss : 0.03328007844058757\n",
      "Validation loss : 0.0012413217773428187\n",
      "Checkpointing new best model in epoch 15 with fold 1\n",
      "Epoch(16/100)\n",
      "Training loss : 0.03235859467469631\n",
      "Validation loss : 0.004114467301405966\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[16], line 34\u001b[0m\n\u001b[1;32m     31\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mprocess_params[\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mprocess\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m]\u001b[39m\u001b[38;5;124m\"\u001b[39m, proces_params[process] )\n\u001b[1;32m     33\u001b[0m P_folder \u001b[38;5;241m=\u001b[39m pids_mat_dir\n\u001b[0;32m---> 34\u001b[0m train_and_save_model(process, P_folder, suffix, third\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m, transfer_bool\u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m, transfer_suffix\u001b[38;5;241m=\u001b[39m suffix_transfer)\n",
      "Cell \u001b[0;32mIn[14], line 233\u001b[0m, in \u001b[0;36mtrain_and_save_model\u001b[0;34m(process, process_folder, suffix, third, transfer_bool, transfer_suffix, process_from_transfer)\u001b[0m\n\u001b[1;32m    230\u001b[0m     hdf5_path \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mTemp_Models/model_weights_\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mprocess\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m_\u001b[39m\u001b[38;5;132;01m{\u001b[39;00msuffix\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.h5\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m    232\u001b[0m \u001b[38;5;66;03m#Train model, best model will be in best_model.pkl as well\u001b[39;00m\n\u001b[0;32m--> 233\u001b[0m model_trained, tr_loss, te_loss \u001b[38;5;241m=\u001b[39m train_model(model, train_dataset, transfer\u001b[38;5;241m=\u001b[39m transfer_bool, hdf5_paths\u001b[38;5;241m=\u001b[39mhdf5_path)\n\u001b[1;32m    235\u001b[0m model_state_dict \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mload(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m./Temp_Models/best_model.pkl\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    236\u001b[0m model\u001b[38;5;241m.\u001b[39mload_state_dict(model_state_dict)\n",
      "Cell \u001b[0;32mIn[14], line 65\u001b[0m, in \u001b[0;36mtrain_model\u001b[0;34m(model, train_dataset, transfer, hdf5_paths)\u001b[0m\n\u001b[1;32m     63\u001b[0m \u001b[38;5;66;03m# Training phase\u001b[39;00m\n\u001b[1;32m     64\u001b[0m model\u001b[38;5;241m.\u001b[39mtrain()\n\u001b[0;32m---> 65\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m batch, (inputs, outputs) \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(train_fold_loader, \u001b[38;5;241m1\u001b[39m):\n\u001b[1;32m     66\u001b[0m     inputs \u001b[38;5;241m=\u001b[39m inputs\u001b[38;5;241m.\u001b[39mto(device)\u001b[38;5;241m.\u001b[39munsqueeze(\u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m     67\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m outputs\u001b[38;5;241m.\u001b[39mto(device)\n",
      "File \u001b[0;32m~/miniconda3/envs/py311/lib/python3.11/site-packages/torch/utils/data/dataloader.py:631\u001b[0m, in \u001b[0;36m_BaseDataLoaderIter.__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    628\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sampler_iter \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    629\u001b[0m     \u001b[38;5;66;03m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[1;32m    630\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reset()  \u001b[38;5;66;03m# type: ignore[call-arg]\u001b[39;00m\n\u001b[0;32m--> 631\u001b[0m data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_next_data()\n\u001b[1;32m    632\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m    633\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dataset_kind \u001b[38;5;241m==\u001b[39m _DatasetKind\u001b[38;5;241m.\u001b[39mIterable \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[1;32m    634\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[1;32m    635\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m>\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called:\n",
      "File \u001b[0;32m~/miniconda3/envs/py311/lib/python3.11/site-packages/torch/utils/data/dataloader.py:675\u001b[0m, in \u001b[0;36m_SingleProcessDataLoaderIter._next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    673\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_next_data\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    674\u001b[0m     index \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_next_index()  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[0;32m--> 675\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dataset_fetcher\u001b[38;5;241m.\u001b[39mfetch(index)  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[1;32m    676\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory:\n\u001b[1;32m    677\u001b[0m         data \u001b[38;5;241m=\u001b[39m _utils\u001b[38;5;241m.\u001b[39mpin_memory\u001b[38;5;241m.\u001b[39mpin_memory(data, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory_device)\n",
      "File \u001b[0;32m~/miniconda3/envs/py311/lib/python3.11/site-packages/torch/utils/data/_utils/fetch.py:49\u001b[0m, in \u001b[0;36m_MapDatasetFetcher.fetch\u001b[0;34m(self, possibly_batched_index)\u001b[0m\n\u001b[1;32m     47\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mauto_collation:\n\u001b[1;32m     48\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m__getitems__\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset\u001b[38;5;241m.\u001b[39m__getitems__:\n\u001b[0;32m---> 49\u001b[0m         data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset\u001b[38;5;241m.\u001b[39m__getitems__(possibly_batched_index)\n\u001b[1;32m     50\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     51\u001b[0m         data \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[idx] \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m possibly_batched_index]\n",
      "File \u001b[0;32m~/miniconda3/envs/py311/lib/python3.11/site-packages/torch/utils/data/dataset.py:399\u001b[0m, in \u001b[0;36mSubset.__getitems__\u001b[0;34m(self, indices)\u001b[0m\n\u001b[1;32m    397\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset\u001b[38;5;241m.\u001b[39m__getitems__([\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mindices[idx] \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m indices])  \u001b[38;5;66;03m# type: ignore[attr-defined]\u001b[39;00m\n\u001b[1;32m    398\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 399\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m [\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mindices[idx]] \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m indices]\n",
      "File \u001b[0;32m~/miniconda3/envs/py311/lib/python3.11/site-packages/torch/utils/data/dataset.py:399\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    397\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset\u001b[38;5;241m.\u001b[39m__getitems__([\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mindices[idx] \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m indices])  \u001b[38;5;66;03m# type: ignore[attr-defined]\u001b[39;00m\n\u001b[1;32m    398\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 399\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m [\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mindices[idx]] \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m indices]\n",
      "File \u001b[0;32m~/miniconda3/envs/py311/lib/python3.11/site-packages/torch/utils/data/dataset.py:206\u001b[0m, in \u001b[0;36mTensorDataset.__getitem__\u001b[0;34m(self, index)\u001b[0m\n\u001b[1;32m    205\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__getitem__\u001b[39m(\u001b[38;5;28mself\u001b[39m, index):\n\u001b[0;32m--> 206\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mtuple\u001b[39m(tensor[index] \u001b[38;5;28;01mfor\u001b[39;00m tensor \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtensors)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    " # Load the .mat file\n",
    "data = scipy.io.loadmat(first_order_processes_mat)\n",
    "\n",
    "# Extract parameter values\n",
    "K_values = data['K_values'][0]\n",
    "T_values = data['T_values'][0]\n",
    "alpha_values = data['alpha_values'][0]\n",
    "Kp_values = data['Kp_values'][0]\n",
    "Ti_values = data['Ti_values'][0]\n",
    "Td_values = data['Td_values'][0]\n",
    "beta_values = data['beta_values'][0]\n",
    "IAE_values = data['IAE_values'][0]\n",
    "tao_values = data['tao_values'][0]\n",
    "L_values = data['L_values'][0]\n",
    "\n",
    "#Default parametrization\n",
    "num_epochs = 100\n",
    "early_stop_thresh = 30\n",
    "suffix = 'NS_test'\n",
    "suffix_transfer = ''\n",
    "nodrop = False\n",
    "prev2 = False\n",
    "custom_loss = False\n",
    "break_first_fold = False\n",
    "\n",
    "for i in [0,1,2,3,4,5,6,7,8,9]: #the 10 processes\n",
    "    process = str(i+1)\n",
    "    print(\"process\", process)\n",
    "    proces_params[process] = [K_values[i], 0, T_values[i], 1, L_values[i]]\n",
    "    beta = beta_values[i]\n",
    "    print(\"process_params['process']\", proces_params[process] )\n",
    "\n",
    "    P_folder = pids_mat_dir\n",
    "    train_and_save_model(process, P_folder, suffix, third=False, transfer_bool= False, transfer_suffix= suffix_transfer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1b103ad-8590-484e-8b88-e9f8ffb90106",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31f105e6-8b88-471d-a0d1-971c03f0826b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
